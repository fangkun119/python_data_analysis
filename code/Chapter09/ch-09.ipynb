{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Textual Data and Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Filtering out stopwords, names, and numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load English stopwords and print some of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(\"Stop words:\", list(sw)[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Gutenberg corpopra and print some of the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gb = nltk.corpus.gutenberg\n",
    "print(\"Gutenberg files:\\n\", gb.fileids()[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Extract sentences from milton-paradise.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_sent = gb.sents(\"milton-paradise.txt\")[:2]\n",
    "print(\"Unfiltered:\", text_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filter out the stopwords from extracted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for sent in text_sent:\n",
    "    filtered = [w for w in sent if w.lower() not in sw]\n",
    "    print(\"Filtered:\\n\", filtered)\n",
    "    tagged = nltk.pos_tag(filtered)\n",
    "    print(\"Tagged:\\n\", tagged)\n",
    "\n",
    "    words= []\n",
    "    for word in tagged:\n",
    "        if word[1] != 'NNP' and word[1] != 'CD':\n",
    "           words.append(word[0]) \n",
    "\n",
    "    print(\"Words:\\n\",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load two documents from NLTK Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hamlet = gb.raw(\"shakespeare-hamlet.txt\")\n",
    "macbeth = gb.raw(\"shakespeare-macbeth.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the feature vector by omitting English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv = sk.feature_extraction.text.CountVectorizer(stop_words='english')\n",
    "print(\"Feature vector:\\n\", cv.fit_transform([hamlet, macbeth]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Print a small selection of the features found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Features:\\n\", cv.get_feature_names()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analyzing word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "gb = nltk.corpus.gutenberg\n",
    "words = gb.words(\"shakespeare-caesar.txt\")\n",
    "\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "filtered = [w.lower() for w in words if w.lower() not in sw and w.lower() not in punctuation]\n",
    "fd = nltk.FreqDist(filtered)\n",
    "print(\"Words\", list(fd.keys())[:5])\n",
    "print(\"Counts\", list(fd.values())[:5])\n",
    "print(\"Max\", fd.max())\n",
    "print(\"Count\", fd['d'])\n",
    "\n",
    "fd = nltk.FreqDist(nltk.bigrams(filtered))\n",
    "print(\"Bigrams\", list(fd.keys())[:5])\n",
    "print(\"Counts\", list(fd.values())[:5])\n",
    "print(\"Bigram Max\", fd.max())\n",
    "print(\"Bigram count\", fd[('let', 'vs')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import random\n",
    "\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def word_features(word):\n",
    "   return {'len': len(word)}\n",
    "\n",
    "def isStopword(word):\n",
    "    return word in sw or word in punctuation\n",
    "gb = nltk.corpus.gutenberg\n",
    "words = gb.words(\"shakespeare-caesar.txt\")\n",
    "\n",
    "labeled_words = ([(word.lower(), isStopword(word.lower())) for \n",
    "word in words])\n",
    "random.seed(42)\n",
    "random.shuffle(labeled_words)\n",
    "print(labeled_words[:5])\n",
    "\n",
    "featuresets = [(word_features(n), word) for (n, word) in \n",
    "labeled_words]\n",
    "cutoff = int(.9 * len(featuresets))\n",
    "train_set, test_set = featuresets[:cutoff], featuresets[cutoff:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"'behold' class\", classifier.classify(word_features('behold')))\n",
    "print(\"'the' class\", classifier.classify(word_features('the')))\n",
    "\n",
    "print(\"Accuracy\", nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "import string\n",
    "\n",
    "labeled_docs = [(list(movie_reviews.words(fid)), cat)\n",
    "        for cat in movie_reviews.categories()\n",
    "        for fid in movie_reviews.fileids(cat)]\n",
    "random.seed(42)\n",
    "random.shuffle(labeled_docs)\n",
    "\n",
    "review_words = movie_reviews.words()\n",
    "print(\"# Review Words\", len(review_words))\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word in sw or word in punctuation\n",
    "\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "print(\"# After filter\", len(filtered))\n",
    "words = FreqDist(filtered)\n",
    "N = int(.05 * len(words.keys()))\n",
    "word_features = list(words.keys())[:N]\n",
    "\n",
    "def doc_features(doc):\n",
    "    doc_words = FreqDist(w for w in doc if not isStopWord(w))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['count (%s)' % word] = (doc_words.get(word, 0))\n",
    "    return features\n",
    "\n",
    "featuresets = [(doc_features(d), c) for (d,c) in labeled_docs]\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy\", accuracy(classifier, test_set))\n",
    "\n",
    "print(classifier.show_most_informative_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word in sw or word in punctuation\n",
    "review_words = movie_reviews.words()\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "\n",
    "words = FreqDist(filtered)\n",
    "N = int(.01 * len(words.keys()))\n",
    "tags = list(words.keys())[:N]\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag, ':', words[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "all_names = set([name.lower() for name in names.words()])\n",
    "\n",
    "def isStopWord(word):\n",
    "    return (word in sw or word in punctuation) or not word.isalpha() or word in all_names\n",
    "\n",
    "review_words = movie_reviews.words()\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "\n",
    "words = FreqDist(filtered)\n",
    "\n",
    "texts = []\n",
    "\n",
    "for fid in movie_reviews.fileids():\n",
    "    texts.append(\" \".join([w.lower() for w in movie_reviews.words(fid) if not isStopWord(w.lower()) and words[w.lower()] > 1]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "matrix = vectorizer.fit_transform(texts)\n",
    "sums = np.array(matrix.sum(axis=0)).ravel()\n",
    "\n",
    "ranks = []\n",
    "\n",
    "for word, val in zip(vectorizer.get_feature_names(), sums):\n",
    "    ranks.append((word, val))\n",
    "\n",
    "df = pd.DataFrame(ranks, columns=[\"term\", \"tfidf\"])\n",
    "df = df.sort_values(['tfidf'])\n",
    "print(df.head())\n",
    "\n",
    "N = int(.01 * len(df))\n",
    "df = df.tail(N)\n",
    "\n",
    "for term, tfidf in zip(df[\"term\"].values, df[\"tfidf\"].values):\n",
    "    print(term, \":\", tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Social Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print([s for s in dir(nx) if s.endswith('graph')])\n",
    "G = nx.davis_southern_women_graph()\n",
    "plt.hist(list(nx.degree(G).values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, node_size=10)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "pos = nx.spring_layout(G, node_size=2)\n",
    "nx.draw(G)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
